{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Lab Task — Preprocessing to Deployment with Twitter Sentiment Dataset\n",
        "\n",
        "## Overview\n",
        "\n",
        "This lab takes students through the **entire NLP pipeline** using a **Twitter Sentiment Analysis dataset** (tweets labeled as positive, negative, or neutral). Students will preprocess the data, represent it using multiple methods, train classical machine learning models, evaluate their results, and finally build a small API for deployment.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "* Apply text preprocessing (regex, case folding, tokenization, stopword removal, punctuation handling).\n",
        "* Compare stemming and lemmatization, and use POS tagging to analyze counts of nouns, verbs, and adjectives.\n",
        "* Build text representations using **Bag-of-Words**, **TF–IDF**, and **Word2Vec**.\n",
        "* Train classical ML models inside an sklearn **Pipeline**.\n",
        "* Evaluate models with standard metrics and perform basic error analysis.\n",
        "* Save a trained pipeline and build a simple API to serve predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will use the **Sentiment140 Twitter dataset** (1.6M tweets, labeled positive/negative). For simplicity, you can work with a smaller balanced subset (e.g., 10k–20k tweets). This dataset is widely used for sentiment analysis and contains real, noisy text with mentions, hashtags, and emoticons.\n",
        "\n",
        "Dataset link: [Sentiment140](http://help.sentiment140.com/for-students/)\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Tasks\n",
        "\n",
        "### 1) Exploratory Data Analysis (EDA)\n",
        "\n",
        "* Load dataset, show class distribution, tweet length histogram, and a few sample tweets.\n",
        "* Deliverable: 3 plots (class balance, length histogram, top frequent tokens) + 5 example tweets.\n",
        "\n",
        "### 2) Regex Cleaning\n",
        "\n",
        "* Remove URLs, mentions (@user), hashtags, emojis (or replace with tokens), and extra whitespace.\n",
        "* Show a before-and-after example for at least 3 tweets.\n",
        "\n",
        "### 3) Case Folding & Tokenization\n",
        "\n",
        "* Convert all text to lowercase.\n",
        "* Tokenize tweets using NLTK or spaCy.\n",
        "* Compare vocabulary size before and after cleaning.\n",
        "\n",
        "### 4) Stopwords, Punctuation, and Numbers\n",
        "\n",
        "* Remove stopwords (with NLTK list, customizable).\n",
        "* Decide how to handle punctuation and numbers (remove or replace with `<NUM>`).\n",
        "* Deliverable: short explanation of your choice.\n",
        "\n",
        "### 5) Stemming vs Lemmatization\n",
        "\n",
        "* Use PorterStemmer (stemming) and spaCy lemmatizer.\n",
        "* Show 20 sample words with their stem vs lemma.\n",
        "* Train quick models with both and compare results.\n",
        "\n",
        "### 6) POS Tagging (extra features)\n",
        "\n",
        "* Use spaCy to count nouns, verbs, and adjectives per tweet.\n",
        "* Add these counts as additional numeric features.\n",
        "* Compare model with vs without POS features.\n",
        "\n",
        "### 7) Text Representation\n",
        "\n",
        "* Implement three representations:\n",
        "  1. **Bag-of-Words** (CountVectorizer)\n",
        "  2. **TF–IDF** (TfidfVectorizer)\n",
        "  3. **Word2Vec** (pretrained embeddings, averaged per tweet)\n",
        "* Compare vocabulary sizes and representation dimensions.\n",
        "\n",
        "### 8) Modeling with Pipelines\n",
        "\n",
        "* Use sklearn Pipelines to connect preprocessing + vectorizer + classifier.\n",
        "* Models to try:\n",
        "  * Multinomial Naive Bayes (for BoW/TF–IDF)\n",
        "  * Logistic Regression\n",
        "  * Linear SVM\n",
        "* Deliverable: comparison table (accuracy, F1-score).\n",
        "\n",
        "### 9) Evaluation & Error Analysis\n",
        "\n",
        "* Use held-out test set.\n",
        "* Metrics: accuracy, precision, recall, F1, confusion matrix.\n",
        "* Show 10 misclassified tweets and discuss why they may be difficult.\n",
        "\n",
        "### 10) Save Model Pipeline\n",
        "\n",
        "* Save the best pipeline using `joblib` or `pickle`.\n",
        "* Deliverable: `sentiment_model.pkl` file.\n",
        "\n",
        "### 11) Build a Simple API (Deployment Step)\n",
        "\n",
        "* Create a small Flask API with one endpoint `/predict` that accepts a tweet as input and returns sentiment prediction.\n",
        "* The API should load the saved pipeline and make predictions.\n",
        "* Deliverable: `app.py` with Flask code + test with a sample curl or Postman request.\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "1. Jupyter Notebook with step-by-step tasks, results, and code.\n",
        "2. Comparison tables for preprocessing choices and representations.\n",
        "3. Saved pipeline file (`.pkl`).\n",
        "4. Flask API script (`app.py`).\n",
        "5. Short report in markdown cell summarizing preprocessing, modeling choices, and results.\n",
        "\n",
        "---\n",
        "\n",
        "This streamlined lab ensures students get hands-on experience with **real-world text preprocessing, classical modeling, and deployment of NLP systems**.\n"
      ],
      "metadata": {
        "id": "dpKLxZ8mNTGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the Dataset\n",
        "\n",
        "We will use the **Sentiment140 Twitter Sentiment dataset**.  \n",
        "It contains 1.6 million tweets labeled as positive (4) or negative (0).  \n",
        "For this lab, we will load a smaller subset (e.g., 20,000 rows) for faster experimentation.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Column order for Sentiment140 dataset:\n",
        "# 0 - target (0 = negative, 4 = positive)\n",
        "# 1 - ids\n",
        "# 2 - date\n",
        "# 3 - flag\n",
        "# 4 - user\n",
        "# 5 - text (tweet)\n",
        "\n",
        "# Update the file path if needed\n",
        "file_path = \"sentiment140.csv\"\n",
        "\n",
        "# Load with proper encoding\n",
        "df = pd.read_csv(file_path, encoding='latin-1', header=None)\n",
        "\n",
        "# Assign column names\n",
        "df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# Map target labels: 0 → negative, 4 → positive\n",
        "df[\"target\"] = df[\"target\"].map({0: \"negative\", 4: \"positive\"})\n",
        "\n",
        "# Take a smaller sample for experiments (optional)\n",
        "df = df.sample(20000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Quick check\n",
        "print(df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "g5JhT6qqMzOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FckTkNZyMqJw",
        "outputId": "0eb78a1b-f5ff-4a8f-b740-ab419b98e42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "(20000, 6)\n",
            "Training set: (1600000, 6)\n",
            "     target          id                          date      flag  \\\n",
            "0  negative  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1  negative  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2  negative  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3  negative  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4  negative  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "              user                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "Test set: (498, 6)\n",
            "     target  id                          date     flag      user  \\\n",
            "0  positive   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
            "1  positive   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
            "2  positive   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
            "3  positive   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
            "4  positive   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
            "\n",
            "                                                text  \n",
            "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
            "1  Reading my kindle2...  Love it... Lee childs i...  \n",
            "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
            "3  @kenburbary You'll love your Kindle2. I've had...  \n",
            "4  @mikefish  Fair enough. But i have the Kindle2...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Step 1: Download the dataset (if not already downloaded)\n",
        "url = \"https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "zip_path = \"trainingandtestdata.zip\"\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    r = requests.get(url)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "# Step 2: Unzip the file\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "# Step 3: Load training data\n",
        "file_path = \"training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, encoding=\"latin-1\", header=None)\n",
        "df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# Map sentiment labels: 0 = negative, 4 = positive\n",
        "df[\"target\"] = df[\"target\"].map({0: \"negative\", 4: \"positive\"})\n",
        "\n",
        "# Optional: sample a smaller dataset for quicker experiments\n",
        "df = df.sample(20000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Preview the data\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "\n",
        "# Load training set\n",
        "train_path = \"training.1600000.processed.noemoticon.csv\"\n",
        "train_df = pd.read_csv(train_path, encoding=\"latin-1\", header=None)\n",
        "train_df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "train_df[\"target\"] = train_df[\"target\"].map({0: \"negative\", 4: \"positive\"})\n",
        "\n",
        "print(\"Training set:\", train_df.shape)\n",
        "print(train_df.head())\n",
        "\n",
        "# Load manual test set\n",
        "test_path = \"testdata.manual.2009.06.14.csv\"\n",
        "test_df = pd.read_csv(test_path, encoding=\"latin-1\", header=None)\n",
        "test_df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# This test set has labels 0 (negative), 2 (neutral), 4 (positive)\n",
        "test_df[\"target\"] = test_df[\"target\"].map({0: \"negative\", 2: \"neutral\", 4: \"positive\"})\n",
        "\n",
        "print(\"Test set:\", test_df.shape)\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZgsBciEMrFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}